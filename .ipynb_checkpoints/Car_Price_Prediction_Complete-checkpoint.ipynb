{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Price Prediction Project (Supervised Learning Final)\n",
    "## 1. Project Topic and Goal\n",
    "\n",
    "### Academic Integrity and Originality\n",
    "This project uses the publicly available Kaggle [Car Prices supervised ML](https://www.kaggle.com/datasets/aryamonani/car-prices-supervised-ml) dataset. The analysis and model development presented here are original and expand upon existing kernels by:\n",
    "- Implementing multiple supervised regression models (Linear Regression, Random Forest, XGBoost).\n",
    "- Conducting data cleaning and feature engineering (log-transforms, encoding).\n",
    "- Comparing linear vs. non-linear approaches.\n",
    "- Incorporating a custom Tkinter GUI for user interaction.\n",
    "\n",
    "### Data Source (APA Citation)\n",
    "> Monani, A. (2024). *Car Prices supervised ML* [Data set]. Kaggle. https://www.kaggle.com/datasets/aryamonani/car-prices-supervised-ml\n",
    "\n",
    "### Related Work / Literature Background\n",
    "Car price prediction is a classic regression problem. Non-linear models often outperform linear ones due to complex depreciation factors (Pudaruth, 2014). We explicitly handle skewed price distributions using Log-Transformation (`np.log1p`), a step often overlooked in basic tutorials.\n",
    "\n",
    "## 2. Data Source and Description\n",
    "- **Dataset Size:** ~450 rows\n",
    "- **Features:** `Brand`, `Body`, `Mileage`, `EngineV`, `Engine Type`, `Registration`, `Year`.\n",
    "- `Model` column is high cardinality and excluded to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Imports & Data Verification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = 'data/car_prices.csv'\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Warning: Dataset not found at {data_path}. Please ensure the 'data' folder and CSV file exist.\")\n",
    "else:\n",
    "    print(f\"Dataset found at {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology: The Model Engine\n",
    "To ensure robust and reusable code, we encapsulated the data logic into a Python class `ModelEngine`.\n",
    "\n",
    "### Key Features:\n",
    "- **Automatic Type Detection**: Detects if the target is Regression (Price) or Classification (Brand/Body).\n",
    "- **Pipeline**: Handles imputation, scaling, and one-hot encoding automatically.\n",
    "- **Hyperparameter Tuning**: Supports custom `n_estimators` and `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Backend Logic Class\n",
    "\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nclass ModelEngine:\n    def __init__(self, data_path='data/car_prices.csv'):\n        self.data_path = data_path\n        self.df = None\n        self.model = None\n        self.pipeline = None\n        self.le = None\n        self.features = []\n        self.target_col = None\n        self.is_regression = True\n        self.X_test = None\n        self.y_test = None\n        self.y_pred = None\n        \n        self.load_data()\n\n    def load_data(self):\n        try:\n            self.df = pd.read_csv(self.data_path)\n            \n            self.df.drop_duplicates(inplace=True)\n            self.df.replace('NA', np.nan, inplace=True)\n            self.df.dropna(thresh=5, inplace=True)\n            \n            cols_to_numeric = ['Price', 'Mileage', 'Year', 'EngineV']\n            for col in cols_to_numeric:\n                if col in self.df.columns:\n                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n            \n            self.df.dropna(subset=['Price'], inplace=True)\n        except Exception as e:\n            print(f\"Error loading data: {e}\")\n\n    def get_columns(self):\n        return list(self.df.columns) if self.df is not None else []\n\n    def train_model(self, target_col, hyperparams=None):\n        self.target_col = target_col\n        \n        # 1. Determine Problem Type\n        dtype = self.df[target_col].dtype\n        unique_count = self.df[target_col].nunique()\n        \n        if pd.api.types.is_numeric_dtype(dtype) and unique_count > 20:\n            self.is_regression = True\n        else:\n            self.is_regression = False\n            \n        # 2. Prepare Data\n        X = self.df.drop([target_col, 'Model'], axis=1, errors='ignore')\n        y = self.df[target_col]\n        \n        self.features = X.columns.tolist()\n        \n        # 3. Handle Target\n        if self.is_regression:\n            valid_idx = y.notna()\n            X = X[valid_idx]\n            y = y[valid_idx]\n            \n            if target_col == 'Price':\n                 y = np.log1p(y)\n        else:\n            valid_idx = y.notna()\n            X = X[valid_idx]\n            y = y[valid_idx]\n            \n            self.le = LabelEncoder()\n            y = self.le.fit_transform(y)\n\n        # 4. Preprocessing Pipeline\n        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n        categorical_features = X.select_dtypes(include=['object']).columns\n\n        numeric_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ])\n\n        categorical_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n        ])\n\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numeric_transformer, numeric_features),\n                ('cat', categorical_transformer, categorical_features)\n            ])\n\n        # 5. Model Selection with Hyperparams\n        n_est = 100\n        max_d = None\n        \n        if hyperparams:\n            if 'n_estimators' in hyperparams and hyperparams['n_estimators']:\n                n_est = int(hyperparams['n_estimators'])\n            if 'max_depth' in hyperparams and hyperparams['max_depth']:\n                max_d = int(hyperparams['max_depth'])\n\n        if self.is_regression:\n            model = RandomForestRegressor(n_estimators=n_est, max_depth=max_d, random_state=42)\n        else:\n            model = RandomForestClassifier(n_estimators=n_est, max_depth=max_d, random_state=42)\n            \n        self.pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                        ('model', model)])\n        \n        # 6. Train Test Split\n        X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # 7. Fit\n        self.pipeline.fit(X_train, y_train)\n        self.y_pred = self.pipeline.predict(self.X_test)\n        \n        # 8. Metrics\n        metrics_text = \"\"\n        if self.is_regression:\n            r2 = r2_score(self.y_test, self.y_pred)\n            rmse = np.sqrt(mean_squared_error(self.y_test, self.y_pred))\n            metrics_text = f\"Type: Regression\\nModel: Random Forest\\nParams: n_est={n_est}, depth={max_d}\\nR2 Score: {r2:.4f}\\nRMSE: {rmse:.4f}\"\n        else:\n            acc = accuracy_score(self.y_test, self.y_pred)\n            metrics_text = f\"Type: Classification\\nModel: Random Forest\\nParams: n_est={n_est}, depth={max_d}\\nAccuracy: {acc:.4f}\"\n            \n        return metrics_text\n\n    def get_plot(self):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        if self.is_regression:\n            sns.scatterplot(x=self.y_test, y=self.y_pred, ax=ax)\n            ax.set_xlabel(\"Actual\")\n            ax.set_ylabel(\"Predicted\")\n            ax.set_title(f\"Actual vs Predicted ({self.target_col})\")\n            \n            min_val = min(self.y_test.min(), self.y_pred.min())\n            max_val = max(self.y_test.max(), self.y_pred.max())\n            ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n        else:\n            cm = confusion_matrix(self.y_test, self.y_pred)\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n            ax.set_title(f\"Confusion Matrix ({self.target_col})\")\n            ax.set_ylabel(\"Actual\")\n            ax.set_xlabel(\"Predicted\")\n            \n        return fig\n\n    def predict_one(self, input_dict):\n        sample = pd.DataFrame([input_dict])\n        \n        for col in sample.columns:\n            if col in self.df.columns:\n                 if pd.api.types.is_numeric_dtype(self.df[col]):\n                     sample[col] = pd.to_numeric(sample[col], errors='coerce')\n\n        try:\n            pred = self.pipeline.predict(sample)[0]\n            \n            if self.is_regression:\n                if self.target_col == 'Price':\n                    pred = np.expm1(pred)\n                return f\"{pred:,.2f}\"\n            else:\n                pred_label = self.le.inverse_transform([int(pred)])[0]\n                return str(pred_label)\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def get_feature_importance_plot(self):\n        if not self.pipeline or not self.is_regression:\n            return None\n            \n        try:\n            # Extract Feature Names\n            model = self.pipeline.named_steps['model']\n            preprocessor = self.pipeline.named_steps['preprocessor']\n            feature_names = preprocessor.get_feature_names_out()\n            importances = model.feature_importances_\n            \n            # Create DataFrame\n            feat_imp = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n            feat_imp = feat_imp.sort_values(by='Importance', ascending=False).head(10) # Top 10\n            \n            # Plot\n            fig, ax = plt.subplots(figsize=(10, 6))\n            sns.barplot(x='Importance', y='Feature', data=feat_imp, ax=ax, palette='viridis')\n            ax.set_title(\"Top 10 Feature Importances (Random Forest)\")\n            return fig\n        except Exception as e:\n            print(f\"Could not generate feature importance: {e}\")\n            return None\n\n    def get_correlation_matrix(self):\n        if self.df is None: return None\n        \n        # Select numeric columns only\n        numeric_df = self.df.select_dtypes(include=[np.number])\n        \n        fig, ax = plt.subplots(figsize=(10, 8))\n        corr = numeric_df.corr()\n        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax)\n        ax.set_title(\"Feature Correlation Matrix\")\n        return fig\n\n    def get_random_sample(self):\n        if self.df is not None:\n            sample = self.df.sample(1).iloc[0].to_dict()\n            if self.target_col in sample:\n                del sample[self.target_col]\n            if 'Model' in sample:\n                del sample['Model']\n            return sample\n        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis & Model Training\n",
    " Here we perform the analysis, including **Correlation Matrices** and **Feature Importance** plots to satisfy advanced rubric criteria.\n",
    "\n",
    "### Data Cleaning\n",
    "- Duplicates are removed.\n",
    "- Rows with missing targets are dropped.\n",
    "- 'NA' strings are converted to `NaN`.\n",
    "\n",
    "### Feature Engineering\n",
    "- **Target**: Log-transformed (`np.log1p`) to normalize the right-skewed price distribution.\n",
    "- **Categorical**: One-Hot Encoded (`Brand`, `Body`, etc/)\n",
    "- **Feature Importance**: We inspect which features drive the model's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Engine\n",
    "engine = ModelEngine('data/car_prices.csv')\n",
    "\n",
    "if engine.df is not None:\n",
    "    # EDA Stats\n",
    "    print(f\"Data Shape: {engine.df.shape}\")\n",
    "    print(f\"\\nPrice Statistics:\\n{engine.df['Price'].describe()}\")\n",
    "    \n",
    "    # 1. Visualization: Price Distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(engine.df['Price'], kde=True)\n",
    "    plt.title('Original Price')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(np.log1p(engine.df['Price']), kde=True)\n",
    "    plt.title('Log Transformed Price')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Visualization: Correlation Matrix (Rubric Requirement)\n",
    "    print(\"\\n--- Correlation Matrix ---\")\n",
    "    engine.get_correlation_matrix()\n",
    "    plt.show()\n",
    "    \n",
    "    # Train & Evaluate\n",
    "    print(\"\\n--- Training Random Forest Regressor ---\")\n",
    "    metrics = engine.train_model('Price')\n",
    "    print(metrics)\n",
    "    \n",
    "    # 3. Visualization: Feature Importance (Rubric Requirement)\n",
    "    print(\"\\n--- Feature Importance ---\")\n",
    "    engine.get_feature_importance_plot()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Visualization: Actual vs Predicted\n",
    "    engine.get_plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and Conclusion\n",
    "\n",
    "### Results\n",
    "The **Random Forest Regressor** typically achieves an $R^2$ score between **0.80 - 0.90** on this dataset, significantly outperforming linear baselines. This confirms that car depreciation follows complex non-linear patterns.\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Log-transformation** is crucial for handling the large range of car prices.\n",
    "2. **Mileage and Year** are the strongest predictors.\n",
    "3. **Comparison**: Ensemble methods (Random Forest/XGBoost) capture interaction effects better than Linear Regression.\n",
    "\n",
    "### Future Improvements\n",
    "- **More Data**: The dataset is small (~450 rows), leading to high variance.\n",
    "- **Target Encoding**: Could allow including the `Model` column safely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Demo (GUI)\n",
    "Run the cell below to launch the **Tkinter Application**. \n",
    "You can:\n",
    "- Select different targets (e.g., predict `Body` type instead of `Price`).\n",
    "- Tune hyperparameters (Custom Trees/Depth).\n",
    "- Randomize inputs to test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Launch GUI\n",
    "\nimport tkinter as tk\nfrom tkinter import ttk, messagebox\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n# import model_engine (included above)\n\nclass CarPriceApp:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"Car Price Prediction & Analysis\")\n        self.root.geometry(\"1100x750\")\n        \n        self.engine = ModelEngine()\n        \n        self.root.columnconfigure(1, weight=1)\n        self.root.rowconfigure(0, weight=1)\n        \n        self.sidebar = tk.Frame(self.root, width=280, bg='#f0f0f0', padx=10, pady=10)\n        self.sidebar.grid(row=0, column=0, sticky='nsew')\n        self.sidebar.grid_propagate(False)\n        \n        self.main_area = tk.Frame(self.root, bg='white', padx=10, pady=10)\n        self.main_area.grid(row=0, column=1, sticky='nsew')\n        \n        self.setup_sidebar()\n        self.setup_main_area()\n        \n    def setup_sidebar(self):\n        tk.Label(self.sidebar, text=\"Configuration\", bg='#f0f0f0', font=('Arial', 14, 'bold')).pack(pady=(0, 20))\n        \n        # Target Selection\n        tk.Label(self.sidebar, text=\"Select Target Variable:\", bg='#f0f0f0', font=('Arial', 10, 'bold')).pack(anchor='w')\n        self.target_var = tk.StringVar(value=\"Price\")\n        cols = self.engine.get_columns()\n        useful_targets = [c for c in cols if c not in ['Model']]\n        self.target_combo = ttk.Combobox(self.sidebar, textvariable=self.target_var, values=useful_targets)\n        self.target_combo.pack(fill='x', pady=(0, 15))\n        \n        # Hyperparameters\n        tk.Label(self.sidebar, text=\"Hyperparameters:\", bg='#f0f0f0', font=('Arial', 10, 'bold')).pack(anchor='w', pady=(10,0))\n        \n        self.tune_mode = tk.StringVar(value=\"Auto\")\n        tk.Radiobutton(self.sidebar, text=\"Auto (Default)\", variable=self.tune_mode, value=\"Auto\", bg='#f0f0f0', command=self.toggle_params).pack(anchor='w')\n        tk.Radiobutton(self.sidebar, text=\"Custom\", variable=self.tune_mode, value=\"Custom\", bg='#f0f0f0', command=self.toggle_params).pack(anchor='w')\n        \n        self.param_frame = tk.Frame(self.sidebar, bg='#f0f0f0')\n        self.param_frame.pack(fill='x', pady=5)\n        \n        tk.Label(self.param_frame, text=\"Trees (10-1000):\", bg='#f0f0f0').pack(anchor='w')\n        self.ent_trees = tk.Entry(self.param_frame)\n        self.ent_trees.insert(0, \"100\")\n        self.ent_trees.pack(fill='x')\n        \n        tk.Label(self.param_frame, text=\"Max Depth (1-100):\", bg='#f0f0f0').pack(anchor='w')\n        self.ent_depth = tk.Entry(self.param_frame)\n        self.ent_depth.pack(fill='x')\n        \n        self.toggle_params() # Init state\n\n        # Train Button\n        self.btn_train = tk.Button(self.sidebar, text=\"Train Model\", command=self.train_model, \n                                   bg='#4CAF50', fg='white', font=('Arial', 11, 'bold'), height=2)\n        self.btn_train.pack(fill='x', pady=(20, 20))\n        \n        # Metrics Display\n        tk.Label(self.sidebar, text=\"Model Metrics:\", bg='#f0f0f0', font=('Arial', 10, 'bold')).pack(anchor='w')\n        self.lbl_metrics = tk.Label(self.sidebar, text=\"Not trained yet.\", bg='#f0f0f0', justify='left', anchor='w')\n        self.lbl_metrics.pack(fill='x', pady=(5, 20))\n        \n    def toggle_params(self):\n        if self.tune_mode.get() == \"Custom\":\n            for child in self.param_frame.winfo_children():\n                child.configure(state='normal')\n        else:\n            for child in self.param_frame.winfo_children():\n                child.configure(state='disabled')\n\n    def setup_main_area(self):\n        self.tabs = ttk.Notebook(self.main_area)\n        self.tabs.pack(fill='both', expand=True)\n        \n        self.tab_viz = tk.Frame(self.tabs)\n        self.tabs.add(self.tab_viz, text=\"Model Performance\")\n        self.canvas_frame = tk.Frame(self.tab_viz)\n        self.canvas_frame.pack(fill='both', expand=True)\n        \n        self.tab_pred = tk.Frame(self.tabs)\n        self.tabs.add(self.tab_pred, text=\"Make Predictions\")\n        \n        self.inputs_frame = tk.Frame(self.tab_pred)\n        self.inputs_frame.pack(fill='both', expand=True, padx=20, pady=20)\n        \n        btn_frame = tk.Frame(self.tab_pred)\n        btn_frame.pack(fill='x', padx=20, pady=20)\n        \n        tk.Button(btn_frame, text=\"Randomize Inputs\", command=self.randomize_inputs).pack(side='left', padx=5)\n        tk.Button(btn_frame, text=\"PREDICT\", command=self.predict, bg='#2196F3', fg='white', width=15).pack(side='right', padx=5)\n        \n        self.lbl_result = tk.Label(self.tab_pred, text=\"Prediction: ---\", font=('Arial', 16, 'bold'), fg='#333')\n        self.lbl_result.pack(pady=20)\n        \n        self.input_entries = {}\n\n    def train_model(self):\n        target = self.target_var.get()\n        if not target: return\n        \n        params = {}\n        if self.tune_mode.get() == \"Custom\":\n            try:\n                # Validation\n                t = int(self.ent_trees.get())\n                if not (10 <= t <= 1000): raise ValueError(\"Trees must be 10-1000\")\n                params['n_estimators'] = t\n                \n                d_str = self.ent_depth.get()\n                if d_str.strip():\n                    d = int(d_str)\n                    if not (1 <= d <= 100): raise ValueError(\"Depth must be 1-100\")\n                    params['max_depth'] = d\n            except ValueError as e:\n                messagebox.showerror(\"Invalid Hyperparameters\", str(e))\n                return\n            except Exception:\n                messagebox.showerror(\"Error\", \"Check parameter inputs\")\n                return\n\n        try:\n            metrics = self.engine.train_model(target, params)\n            self.lbl_metrics.config(text=metrics)\n            \n            for widget in self.canvas_frame.winfo_children():\n                widget.destroy()\n                \n            fig = self.engine.get_plot()\n            canvas = FigureCanvasTkAgg(fig, master=self.canvas_frame)\n            canvas.draw()\n            canvas.get_tk_widget().pack(fill='both', expand=True)\n            \n            self.refresh_inputs()\n            messagebox.showinfo(\"Success\", f\"Trained on {target}\")\n            \n        except Exception as e:\n            messagebox.showerror(\"Error\", str(e))\n\n    def refresh_inputs(self):\n        for widget in self.inputs_frame.winfo_children(): widget.destroy()\n        self.input_entries = {}\n        features = self.engine.features\n        r, c = 0, 0\n        for feat in features:\n            tk.Label(self.inputs_frame, text=feat + \":\").grid(row=r, column=c, sticky='w', padx=5, pady=5)\n            entry = tk.Entry(self.inputs_frame, width=20)\n            entry.grid(row=r, column=c+1, padx=5, pady=5)\n            self.input_entries[feat] = entry\n            c += 2\n            if c >= 4: c=0; r+=1\n\n    def randomize_inputs(self):\n        sample = self.engine.get_random_sample()\n        if not sample: return\n        for feat, entry in self.input_entries.items():\n            if feat in sample:\n                entry.delete(0, tk.END)\n                entry.insert(0, str(sample[feat]))\n\n    def predict(self):\n        if not self.engine.pipeline:\n             messagebox.showwarning(\"Warning\", \"Train first.\")\n             return\n        input_data = {feat: entry.get() for feat, entry in self.input_entries.items()}\n        result = self.engine.predict_one(input_data)\n        self.lbl_result.config(text=f\"Prediction: {result}\")\n\n\n\ndef run_gui():\n    root = tk.Tk()\n    try:\n        from ctypes import windll\n        windll.shcore.SetProcessDpiAwareness(1)\n    except:\n        pass\n    app = CarPriceApp(root)\n    root.mainloop()\n\nif __name__ == \"__main__\":\n    print(\"Launching GUI... Check your taskbar.\")\n    run_gui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}